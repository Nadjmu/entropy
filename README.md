# Entropy as the effective number of internal independent degrees of freedom 
My most recent understanding of entropy and the logarithm log(A) in general is as the effective number of internal independent degrees of freedom of the multiplicative structure that is A. This tool calculates the entropy of many well-known distributions depending on their parameters that uniquely define them. The idea is to look at the behaviour of entropy in the parameter space and get an intuition on how and how many internal independent degrees of freedom are hidden across the parameter manifold. The entropy is maximized leading to a flow on the parameter manifold which directly translates to an evolution of the distribution.   
https://entropyflow.streamlit.app/
